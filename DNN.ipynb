{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda97eb7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffec810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9a74f",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33510ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Calculates and returns the depth of the tree\n",
    "        Parameters:\n",
    "            tree (<DecisionTreeClassifier> object): tree object to draw (optional)\n",
    "        Returns:\n",
    "            (int): depth of the tree\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a3de9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Computes ReLU (Rectified Lenear Unit) activation on Z.\n",
    "        Parameters:\n",
    "            Z (<numpy.ndarray>)\n",
    "        Returns:\n",
    "            A (<numpy.ndarray>): Z passed to the relu\n",
    "            cache (<numpy.ndarray>): input (for backward propagation)\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    \n",
    "    return (A, cache)\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Computes sigmoid activation on Z.\n",
    "        Parameters:\n",
    "            Z (<numpy.ndarray>)\n",
    "        Returns:\n",
    "            A (<numpy.ndarray>): Z passed to the relu\n",
    "            cache (<numpy.ndarray>): input (for backward propagation)\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return (A, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6a77f",
   "metadata": {},
   "source": [
    "## Weights Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209c68e",
   "metadata": {},
   "source": [
    "### add diff inits\n",
    "He, Xavier, random, zeros?\n",
    "\n",
    "https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd200f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes the weights for the (deep) neural network layers using Xavier's Initialization.\n",
    "        Parameters:\n",
    "            layer_dims (list): list of layers' number of nodes (including input layer)\n",
    "        Returns:\n",
    "            params (dict): dictionary containing weights and bias per layer\n",
    "                \"Wn\": <numpy.ndarray> weights for layer n\n",
    "                \"bn\": <numpy.ndarray> bias for layer n\n",
    "    \"\"\"\n",
    "    param = {}\n",
    "    nlayers = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, nlayers):\n",
    "        params[f\"W{l}\"] = np.random.rand(layer_dims[l], layer_dims[l-1]) \\\n",
    "        * np.sqrt(6/(layer_dims[l]+layer_dims[l-1]))\n",
    "#         params[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) \\\n",
    "#         * np.sqrt(2/(layer_dims[l]+layer_dims[l-1]))\n",
    "        \n",
    "        params[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411f729",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bfcb5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate_layer(A_prev, W, b, activate_func):\n",
    "    \"\"\"\n",
    "    Applies forward propagation (linear & activation).\n",
    "    Parameters:\n",
    "            A_prev (list): this layer's input (last layer's output)\n",
    "            params (dict): dictionary containing weights and bias per layer\n",
    "                \"Wn\": <numpy.ndarray> weights for layer n\n",
    "                \"bn\": <numpy.ndarray> bias for layer n\n",
    "            activate_func (<function>): activation function\n",
    "        Returns:\n",
    "            A (<numpy.ndarray>): layer output (post-activation)\n",
    "            cache (tuple): forward propagation caches for backward\n",
    "                (linear_cache, activation_cache)\n",
    "            \n",
    "    \"\"\"\n",
    "    Z = W @ A_prev + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    \n",
    "    A, activation_cache = activate_func(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return (A, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1911cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, params):\n",
    "    \"\"\"\n",
    "    Applies forward propagation (linear & activation).\n",
    "    Parameters:\n",
    "            X (list): this layer's input (last layer's output)\n",
    "            params (dict): dictionary containing weights and bias per layer\n",
    "                \"Wn\": <numpy.ndarray> weights for layer n\n",
    "                \"bn\": <numpy.ndarray> bias for layer n\n",
    "            activate_func (<function>): activation function\n",
    "        Returns:\n",
    "            A (<numpy.ndarray>): layer output (post-activation)\n",
    "            cache (tuple): forward propagation caches for backward\n",
    "                (linear_cache, activation_cache)\n",
    "            \n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    nlayers = len(params) // 3\n",
    "    \n",
    "    for l in range(1, nlayers+1):\n",
    "        A, cache = forward_propagate_layer(A, \n",
    "                                           params[f\"W{l}\"], \n",
    "                                           params[f\"b{l}\"],\n",
    "                                           params[f\"A{l}\"])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    \n",
    "    return (A, caches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
