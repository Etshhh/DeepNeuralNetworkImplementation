{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda97eb7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffec810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9a74f",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33510ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCalculates and returns the depth of the tree\\n    Parameters:\\n        tree (<DecisionTreeClassifier> object): tree object to draw (optional)\\n    Returns:\\n        (int): depth of the tree\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Calculates and returns the depth of the tree\n",
    "        Parameters:\n",
    "            tree (<DecisionTreeClassifier> object): tree object to draw (optional)\n",
    "        Returns:\n",
    "            (int): depth of the tree\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3de9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Computes ReLU (Rectified Lenear Unit) activation on Z.\n",
    "        Parameters:\n",
    "            Z (<numpy.ndarray>)\n",
    "        Returns:\n",
    "            A (<numpy.ndarray>): Z passed to the relu\n",
    "            cache (<numpy.ndarray>): input (for backward propagation)\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    \n",
    "    return (A, cache)\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Computes sigmoid activation on Z.\n",
    "        Parameters:\n",
    "            Z (<numpy.ndarray>)\n",
    "        Returns:\n",
    "            A (<numpy.ndarray>): Z passed to the relu\n",
    "            cache (<numpy.ndarray>): input (for backward propagation)\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return (A, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdd07a",
   "metadata": {},
   "source": [
    "## Weights Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0b345",
   "metadata": {},
   "source": [
    "### add diff inits\n",
    "He, Xavier, random, zeros?\n",
    "\n",
    "https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56cbc75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes the weights for the (deep) neural network layers using Xavier's Initialization.\n",
    "        Parameters:\n",
    "            layer_dims (list): list of layers' number of nodes (including input layer)\n",
    "        Returns:\n",
    "            params (dict): dictionary containing weights and bias per layer\n",
    "                \"Wn\": <numpy.ndarray> weights for layer n\n",
    "                \"bn\": <numpy.ndarray> bias for layer n\n",
    "    \"\"\"\n",
    "    param = {}\n",
    "    nlayers = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, nlayers):\n",
    "        params[f\"W{l}\"] = np.random.rand(layer_dims[l], layer_dims[l-1]) \\\n",
    "        * np.sqrt(6/(layer_dims[l]+layer_dims[l-1]))\n",
    "#         params[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) \\\n",
    "#         * np.sqrt(2/(layer_dims[l]+layer_dims[l-1]))\n",
    "        \n",
    "        params[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eab0d8",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b89aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate_layer(A_prev, W, b, activate_func):\n",
    "    \"\"\"\n",
    "    Applies forward propagation (linear & activation).\n",
    "    Parameters:\n",
    "            A_prev (list): this layer's input (last layer's output)\n",
    "            params (dict): dictionary containing weights and bias per layer\n",
    "                \"Wn\": <numpy.ndarray> weights for layer n\n",
    "                \"bn\": <numpy.ndarray> bias for layer n\n",
    "                \"An\": (<function>): activation function\n",
    "        Returns:\n",
    "            A (<numpy.ndarray>): layer output (post-activation)\n",
    "            cache (tuple): forward propagation caches for backward\n",
    "                (linear_cache, (activation_cache, activation_name))\n",
    "            \n",
    "    \"\"\"\n",
    "    Z = W @ A_prev + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    \n",
    "    A, activation_cache = activate_func(Z)\n",
    "    cache = (linear_cache, (activation_cache, activate_func.__name__))\n",
    "    \n",
    "    return (A, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430f722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, params):\n",
    "    \"\"\"\n",
    "    Forward propagates X through all model layers.\n",
    "    Parameters:\n",
    "            X (list): this layer's input (last layer's output)\n",
    "            params (dict): dictionary containing weights and bias per layer\n",
    "                \"Wn\": <numpy.ndarray> weights for layer n\n",
    "                \"bn\": <numpy.ndarray> bias for layer n\n",
    "                \"An\": (<function>): activation function\n",
    "        Returns:\n",
    "            A (<numpy.ndarray>): model output\n",
    "            cache (list): forward propagation caches for backward\n",
    "                [(linear_cache, activation_cache), ...]\n",
    "            \n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    nlayers = len(params) // 3\n",
    "    \n",
    "    for l in range(1, nlayers+1):\n",
    "        A, cache = forward_propagate_layer(A, \n",
    "                                           params[f\"W{l}\"], \n",
    "                                           params[f\"b{l}\"],\n",
    "                                           params[f\"A{l}\"])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    return (A, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98712a9",
   "metadata": {},
   "source": [
    "## Cost Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b6920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Yh, Y):\n",
    "    \"\"\"\n",
    "    Computes cost using the cross-entropy / log-loss function\n",
    "    Parameters:\n",
    "            Yh (<numpy.ndarray>): predicted output (y_hat)\n",
    "            Y (<numpy.ndarray>): true output (y)\n",
    "        Returns:\n",
    "            cost (float): cost value\n",
    "    \"\"\"\n",
    "    cost = ((Y @ np.log(Yh.T)) + ((1 - Y) @ np.log((1-Yh).T))) / (-Y.shape[1])\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20156b9",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eaf96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate_layer(dA, cache):\n",
    "    \"\"\"\n",
    "    Applies backward propagation (linear & activation).\n",
    "    Parameters:\n",
    "            dA (<numpy.ndarray>): current layer's post-activation gradient \n",
    "            cache (tuple): forward propagation caches for backward\n",
    "                (linear_cache, (activation_cache, activation_name))\n",
    "        Returns:\n",
    "            dA_prev (<numpy.ndarray>): Gradient with respect to previous layer's input (A_prev)\n",
    "            dW (<numpy.ndarray>): Gradient with respect to current layer's wieghts (W)\n",
    "            db (<numpy.ndarray>): Gradient with respect to previous layer's bias (b)\n",
    "            \n",
    "    \"\"\"\n",
    "    def relu_backward(dA, cache):\n",
    "        \"\"\"\n",
    "        ReLU backward propagation implementation.\n",
    "        Parameters:\n",
    "            dA (<numpy.ndarray>): post-activation gradient \n",
    "            Y (<numpy.ndarray>): activation input (Z)\n",
    "        Returns:\n",
    "            dZ (<numpy.ndarray>): Gradient with respect to activation input (Z)\n",
    "        \"\"\"\n",
    "        dZ = np.copy(dA)\n",
    "        dZ[cache <= 0] = 0\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def sigmoid_backward(dA, cache):\n",
    "        \"\"\"\n",
    "        sigmoid backward propagation implementation.\n",
    "        Parameters:\n",
    "            dA (<numpy.ndarray>): post-activation gradient\n",
    "            Y (<numpy.ndarray>): activation input (Z)\n",
    "        Returns:\n",
    "            dZ (<numpy.ndarray>): Gradient with respect to activation input (Z)\n",
    "        \"\"\"\n",
    "        s, _ = sigmoid(cache)\n",
    "        dZ = dA * s * (1 - s)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    activation_backward_func = {'relu': relu_backward,\n",
    "                                'sigmoid': sigmoid_backward}\n",
    "\n",
    "    linear_cache, (activation_cache, activation_name) = cache\n",
    "    \n",
    "    # Activation backward propagation\n",
    "    dZ = activation_backward_func[activation_name](dA, activation_cache)\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    # Linear backward propagation\n",
    "    dA_prev = W.T @ dZ\n",
    "    dW = (dZ @ A_prev.T) / m\n",
    "    db = np.sum(dZ, 1, keepdims=True) / m\n",
    "    \n",
    "    return (dA_prev, dW, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8bade54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backward_propagate(Yh, Y, caches):\n",
    "    \"\"\"\n",
    "    Backward propagates Error through all model layers.\n",
    "    Parameters:\n",
    "            Yh (<numpy.ndarray>): predicted output (y_hat)\n",
    "            Y (<numpy.ndarray>): true output (y)\n",
    "            cache (list): forward propagation caches\n",
    "                [(linear_cache, activation_cache), ...]\n",
    "        Returns:\n",
    "            grads (dict): dictionary containing parameters' gradients\n",
    "                \"dAn\": <numpy.ndarray> weights for layer n (*deprecated)\n",
    "                \"dWn\": <numpy.ndarray> weights for layer n\n",
    "                \"dbn\": <numpy.ndarray> bias for layer n      \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    nlayers = len(caches)\n",
    "    \n",
    "    grads[f\"dA{nlayers}\"] = (Yh - Y) / ((1 - Yh) * Yh)\n",
    "    \n",
    "    for l in range(nlayers, 0, -1):\n",
    "        current_cache = caches[l-1]\n",
    "        dA_prev, dW, db = backward_propagate_layer(grads[f\"dA{l}\"], \n",
    "                                                   current_cache)\n",
    "        grads[f\"dA{l-1}\"] = dA_prev\n",
    "        grads[f\"dW{l}\"] = dW\n",
    "        grads[f\"db{l}\"] = db\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f86ce",
   "metadata": {},
   "source": [
    "## Update Parameters (with Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06b4c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, lr):\n",
    "    \"\"\"\n",
    "    Apply Gradient Descent to update parameters using \n",
    "        computed gradients and learning rate.\n",
    "    Parameters:\n",
    "        params (dict): dictionary containing weights and bias per layer\n",
    "                    \"Wn\": <numpy.ndarray> weights for layer n\n",
    "                    \"bn\": <numpy.ndarray> bias for layer n\n",
    "                    \"An\": (<function>): activation function\n",
    "        grads (dict): dictionary containing parameters' gradients\n",
    "                    \"dAn\": <numpy.ndarray> weights for layer n (*deprecated)\n",
    "                    \"dWn\": <numpy.ndarray> weights for layer n\n",
    "                    \"dbn\": <numpy.ndarray> bias for layer n \n",
    "        lr (float): learning rate\n",
    "    Returns:\n",
    "        params (dict): *updated dictionary containing weights and bias per layer\n",
    "                    \"Wn\": <numpy.ndarray> weights for layer n\n",
    "                    \"bn\": <numpy.ndarray> bias for layer n\n",
    "                    \"An\": (<function>): activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    nlayers = len(params) // 3\n",
    "    print(nlayers)\n",
    "    for l in range(1, nlayers+1):\n",
    "        params[f\"W{l}\"] -= lr * grads[f\"dW{l}\"]\n",
    "        params[f\"b{l}\"] -= lr * grads[f\"db{l}\"]\n",
    "    \n",
    "    return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
